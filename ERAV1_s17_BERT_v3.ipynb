{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3zCTvXkd7hdT",
        "outputId": "92d99c17-3f40-43f0-d1fe-02a307c6cb7e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fri Sep 15 07:49:07 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 525.105.17   Driver Version: 525.105.17   CUDA Version: 12.0     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   65C    P8    11W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/anilbhatt1/ERA1_S17_BERT_GPT_VIT.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R7CFAP76dGWk",
        "outputId": "285160b5-8be2-488c-cb71-5f647d02aa94"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'ERA1_S17_BERT_GPT_VIT'...\n",
            "remote: Enumerating objects: 54, done.\u001b[K\n",
            "remote: Counting objects: 100% (54/54), done.\u001b[K\n",
            "remote: Compressing objects: 100% (44/44), done.\u001b[K\n",
            "remote: Total 54 (delta 14), reused 47 (delta 7), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (54/54), 4.47 MiB | 4.69 MiB/s, done.\n",
            "Resolving deltas: 100% (14/14), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "print(os.getcwd())\n",
        "os.chdir('/content/ERA1_S17_BERT_GPT_VIT/assignment')\n",
        "print(os.getcwd())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LyfJs0iDphAO",
        "outputId": "58c4af51-9c41-40ec-805a-9fc3664b20b2"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "/content/ERA1_S17_BERT_GPT_VIT/assignment\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -r requirements.txt -q"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uyWCBaVmprXk",
        "outputId": "98a64ea3-82fe-4824-84f3-ce5cbbe9746d"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m59.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m294.8/294.8 kB\u001b[0m \u001b[31m26.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m69.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m56.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset\n",
        "import torch.nn.functional as F\n",
        "from collections import Counter\n",
        "from os.path import exists\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import random\n",
        "import torch\n",
        "import math\n",
        "import re\n",
        "import yaml\n",
        "from transformer import *\n",
        "from bert_data import *"
      ],
      "metadata": {
        "id": "oiLFoaAzp3bb"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the YAML file\n",
        "with open('/content/ERA1_S17_BERT_GPT_VIT/assignment/config.yaml', 'r') as yaml_file:\n",
        "    cfg = yaml.load(yaml_file, Loader=yaml.FullLoader)\n",
        "cfg['model'] = 'bert'\n",
        "model_nm = cfg['model']\n",
        "print(model_nm)\n",
        "cfg"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z3-xeWSdqDY6",
        "outputId": "a116f1e8-cbd4-4043-ed28-a27ab188b23e"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "bert\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'model': 'bert',\n",
              " 'bert': {'batch_size': 1024,\n",
              "  'seq_len': 20,\n",
              "  'embed_size': 128,\n",
              "  'inner_ff_size': 512,\n",
              "  'n_heads': 8,\n",
              "  'n_code': 8,\n",
              "  'n_vocab': 40000,\n",
              "  'dropout': 0.1,\n",
              "  'n_workers': 12,\n",
              "  'optim_kwargs': {'lr': 0.0001,\n",
              "   'weight_decay': 0.0001,\n",
              "   'betas': [0.9, 0.999]},\n",
              "  'sentence_path': '/content/ERA1_S17_BERT_GPT_VIT/course_docs/BERT/training.txt',\n",
              "  'vocab_path': '/content/ERA1_S17_BERT_GPT_VIT/course_docs/BERT/vocab.txt',\n",
              "  'data_kwargs': {'shuffle': True,\n",
              "   'drop_last': False,\n",
              "   'pin_memory': True,\n",
              "   'batch_size': 1024},\n",
              "  'train_iter': 2000,\n",
              "  'print_iter': 10,\n",
              "  'embed_save_limit': 3000}}"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('initializing from config data..')\n",
        "batch_size = cfg[model_nm]['batch_size']\n",
        "seq_len = cfg[model_nm]['seq_len']\n",
        "embed_size = cfg[model_nm]['embed_size']\n",
        "inner_ff_size = cfg[model_nm]['inner_ff_size']\n",
        "n_heads = cfg[model_nm]['n_heads']\n",
        "n_code = cfg[model_nm]['n_code']\n",
        "n_vocab = cfg[model_nm]['n_vocab']\n",
        "dropout = cfg[model_nm]['dropout']\n",
        "# n_workers = cfg[model_nm]['n_workers']\n",
        "sent_path = cfg[model_nm]['sentence_path']\n",
        "vocab_path = cfg[model_nm]['vocab_path']\n",
        "\n",
        "#optimizer\n",
        "optim_kwargs = cfg[model_nm]['optim_kwargs']\n",
        "data_kwargs = cfg[model_nm]['data_kwargs']\n",
        "train_iter = cfg[model_nm]['train_iter']\n",
        "print_iter = cfg[model_nm]['print_iter']\n",
        "embed_save_limit = cfg[model_nm]['embed_save_limit']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fcJyCw6vqJ59",
        "outputId": "d2e0eba3-e86d-49e3-fa59-4146b6645fe5"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "initializing from config data..\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# Input\n",
        "# =============================================================================\n",
        "#1) load text\n",
        "print('loading text...')\n",
        "sentences = open(sent_path).read().lower().split('\\n')\n",
        "\n",
        "#2) tokenize sentences (can be done during training, you can also use spacy udpipe)\n",
        "print('tokenizing sentences...')\n",
        "special_chars = ',?;.:/*!+-()[]{}\"\\'&'\n",
        "sentences = [re.sub(f'[{re.escape(special_chars)}]', ' \\g<0> ', s).split(' ') for s in sentences]\n",
        "sentences = [[w for w in s if len(w)] for s in sentences]\n",
        "\n",
        "#3) create vocab if not already created\n",
        "print('creating/loading vocab...')\n",
        "if not exists(vocab_path):\n",
        "    print('vocab_path doesnt exist...')\n",
        "    words = [w for s in sentences for w in s]\n",
        "    vocab = Counter(words).most_common(n_vocab) #keep the N most frequent words\n",
        "    vocab = [w[0] for w in vocab]\n",
        "    open(vocab_path, 'w+').write('\\n'.join(vocab))\n",
        "else:\n",
        "    print('vocab_path exists...')\n",
        "    vocab = open(vocab_path).read().split('\\n')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nk2d-25K8Fcy",
        "outputId": "f44711d1-6885-43a7-b065-9ee5c96b9791"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loading text...\n",
            "tokenizing sentences...\n",
            "creating/loading vocab...\n",
            "vocab_path exists...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#4) create dataset\n",
        "print('creating dataset...')\n",
        "dataset = SentencesDataset(sentences, vocab, seq_len)\n",
        "kwargs = cfg[model_nm]['data_kwargs']\n",
        "data_loader = torch.utils.data.DataLoader(dataset, **kwargs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "68IDmXOJ8Ls1",
        "outputId": "89f8b51a-ff25-4473-f809-429e8a86afc0"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "creating dataset...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# Model\n",
        "# =============================================================================\n",
        "#init model\n",
        "print('initializing model...')\n",
        "model = bert_Transformer(n_code, n_heads, embed_size, inner_ff_size, len(dataset.vocab), seq_len, dropout)\n",
        "model = model.cuda()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2nxPTLO98N6f",
        "outputId": "38ae6b53-42da-4448-8772-d158f1ef5c6a"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "initializing model...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# Optimizer\n",
        "# =============================================================================\n",
        "print('initializing optimizer and loss...')\n",
        "optimizer = optim.Adam(model.parameters(), **optim_kwargs)\n",
        "loss_model = nn.CrossEntropyLoss(ignore_index=dataset.IGNORE_IDX)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tpRFlrtM8P8F",
        "outputId": "cbfc828b-7c9a-45a1-8979-1cfd261ad655"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "initializing optimizer and loss...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# Train\n",
        "# =============================================================================\n",
        "print('training...')\n",
        "model.train()\n",
        "batch_iter = iter(data_loader)\n",
        "for it in range(train_iter):\n",
        "\n",
        "    #get batch\n",
        "    batch, batch_iter = get_batch(data_loader, batch_iter)\n",
        "\n",
        "    #infer\n",
        "    masked_input = batch['input']\n",
        "    masked_target = batch['target']\n",
        "\n",
        "    masked_input = masked_input.cuda(non_blocking=True)\n",
        "    masked_target = masked_target.cuda(non_blocking=True)\n",
        "    output = model(masked_input)\n",
        "\n",
        "    #compute the cross entropy loss\n",
        "    output_v = output.view(-1,output.shape[-1])\n",
        "    target_v = masked_target.view(-1,1).squeeze()\n",
        "    loss = loss_model(output_v, target_v)\n",
        "\n",
        "    #compute gradients\n",
        "    loss.backward()\n",
        "\n",
        "    #apply gradients\n",
        "    optimizer.step()\n",
        "\n",
        "    #print step\n",
        "    if it % print_iter == 0:\n",
        "        print('it:', it,\n",
        "              ' | loss', np.round(loss.item(),2),\n",
        "              ' | Δw:', round(model.embeddings.weight.grad.abs().sum().item(),3))\n",
        "\n",
        "    #reset gradients\n",
        "    optimizer.zero_grad()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1u7Y4RQl8UfG",
        "outputId": "9072330f-4fd9-44fb-8247-cc0eccb7ebec"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training...\n",
            "it: 0  | loss 10.32  | Δw: 1.144\n",
            "it: 10  | loss 9.57  | Δw: 0.547\n",
            "it: 20  | loss 9.34  | Δw: 0.343\n",
            "it: 30  | loss 9.17  | Δw: 0.264\n",
            "it: 40  | loss 9.0  | Δw: 0.231\n",
            "it: 50  | loss 8.82  | Δw: 0.2\n",
            "it: 60  | loss 8.72  | Δw: 0.181\n",
            "it: 70  | loss 8.51  | Δw: 0.17\n",
            "it: 80  | loss 8.36  | Δw: 0.169\n",
            "it: 90  | loss 8.24  | Δw: 0.161\n",
            "it: 100  | loss 8.11  | Δw: 0.148\n",
            "it: 110  | loss 7.86  | Δw: 0.151\n",
            "it: 120  | loss 7.77  | Δw: 0.136\n",
            "it: 130  | loss 7.67  | Δw: 0.132\n",
            "it: 140  | loss 7.53  | Δw: 0.132\n",
            "it: 150  | loss 7.41  | Δw: 0.125\n",
            "it: 160  | loss 7.34  | Δw: 0.121\n",
            "it: 170  | loss 7.24  | Δw: 0.115\n",
            "it: 180  | loss 7.1  | Δw: 0.116\n",
            "it: 190  | loss 7.09  | Δw: 0.113\n",
            "it: 200  | loss 6.92  | Δw: 0.11\n",
            "it: 210  | loss 6.94  | Δw: 0.111\n",
            "it: 220  | loss 6.87  | Δw: 0.108\n",
            "it: 230  | loss 6.82  | Δw: 0.109\n",
            "it: 240  | loss 6.79  | Δw: 0.107\n",
            "it: 250  | loss 6.67  | Δw: 0.109\n",
            "it: 260  | loss 6.61  | Δw: 0.112\n",
            "it: 270  | loss 6.56  | Δw: 0.112\n",
            "it: 280  | loss 6.57  | Δw: 0.117\n",
            "it: 290  | loss 6.58  | Δw: 0.113\n",
            "it: 300  | loss 6.44  | Δw: 0.11\n",
            "it: 310  | loss 6.45  | Δw: 0.122\n",
            "it: 320  | loss 6.45  | Δw: 0.125\n",
            "it: 330  | loss 6.4  | Δw: 0.133\n",
            "it: 340  | loss 6.46  | Δw: 0.123\n",
            "it: 350  | loss 6.35  | Δw: 0.125\n",
            "it: 360  | loss 6.45  | Δw: 0.126\n",
            "it: 370  | loss 6.35  | Δw: 0.159\n",
            "it: 380  | loss 6.46  | Δw: 0.143\n",
            "it: 390  | loss 6.35  | Δw: 0.147\n",
            "it: 400  | loss 6.38  | Δw: 0.154\n",
            "it: 410  | loss 6.31  | Δw: 0.156\n",
            "it: 420  | loss 6.39  | Δw: 0.178\n",
            "it: 430  | loss 6.46  | Δw: 0.179\n",
            "it: 440  | loss 6.41  | Δw: 0.201\n",
            "it: 450  | loss 6.38  | Δw: 0.233\n",
            "it: 460  | loss 6.34  | Δw: 0.241\n",
            "it: 470  | loss 6.35  | Δw: 0.247\n",
            "it: 480  | loss 6.37  | Δw: 0.276\n",
            "it: 490  | loss 6.36  | Δw: 0.296\n",
            "it: 500  | loss 6.33  | Δw: 0.295\n",
            "it: 510  | loss 6.37  | Δw: 0.296\n",
            "it: 520  | loss 6.26  | Δw: 0.323\n",
            "it: 530  | loss 6.34  | Δw: 0.37\n",
            "it: 540  | loss 6.25  | Δw: 0.393\n",
            "it: 550  | loss 6.23  | Δw: 0.414\n",
            "it: 560  | loss 6.28  | Δw: 0.457\n",
            "it: 570  | loss 6.3  | Δw: 0.474\n",
            "it: 580  | loss 6.25  | Δw: 0.488\n",
            "it: 590  | loss 6.35  | Δw: 0.604\n",
            "it: 600  | loss 6.31  | Δw: 0.594\n",
            "it: 610  | loss 6.29  | Δw: 0.579\n",
            "it: 620  | loss 6.28  | Δw: 0.609\n",
            "it: 630  | loss 6.3  | Δw: 0.679\n",
            "it: 640  | loss 6.24  | Δw: 0.709\n",
            "it: 650  | loss 6.22  | Δw: 0.717\n",
            "it: 660  | loss 6.24  | Δw: 0.711\n",
            "it: 670  | loss 6.28  | Δw: 0.744\n",
            "it: 680  | loss 6.26  | Δw: 0.835\n",
            "it: 690  | loss 6.28  | Δw: 0.895\n",
            "it: 700  | loss 6.31  | Δw: 0.901\n",
            "it: 710  | loss 6.27  | Δw: 0.927\n",
            "it: 720  | loss 6.27  | Δw: 0.879\n",
            "it: 730  | loss 6.22  | Δw: 0.873\n",
            "it: 740  | loss 6.23  | Δw: 0.932\n",
            "it: 750  | loss 6.24  | Δw: 0.882\n",
            "it: 760  | loss 6.17  | Δw: 0.966\n",
            "it: 770  | loss 6.17  | Δw: 1.049\n",
            "it: 780  | loss 6.21  | Δw: 1.028\n",
            "it: 790  | loss 6.33  | Δw: 1.013\n",
            "it: 800  | loss 6.17  | Δw: 1.111\n",
            "it: 810  | loss 6.14  | Δw: 1.162\n",
            "it: 820  | loss 6.21  | Δw: 1.196\n",
            "it: 830  | loss 6.16  | Δw: 1.34\n",
            "it: 840  | loss 6.13  | Δw: 1.231\n",
            "it: 850  | loss 6.17  | Δw: 1.259\n",
            "it: 860  | loss 6.18  | Δw: 1.207\n",
            "it: 870  | loss 6.15  | Δw: 1.259\n",
            "it: 880  | loss 6.19  | Δw: 1.243\n",
            "it: 890  | loss 6.21  | Δw: 1.328\n",
            "it: 900  | loss 6.18  | Δw: 1.298\n",
            "it: 910  | loss 6.15  | Δw: 1.307\n",
            "it: 920  | loss 6.21  | Δw: 1.444\n",
            "it: 930  | loss 6.09  | Δw: 1.43\n",
            "it: 940  | loss 6.13  | Δw: 1.501\n",
            "it: 950  | loss 6.2  | Δw: 1.362\n",
            "it: 960  | loss 6.18  | Δw: 1.383\n",
            "it: 970  | loss 6.16  | Δw: 1.454\n",
            "it: 980  | loss 6.08  | Δw: 1.433\n",
            "it: 990  | loss 6.04  | Δw: 1.544\n",
            "it: 1000  | loss 6.15  | Δw: 1.476\n",
            "it: 1010  | loss 6.04  | Δw: 1.624\n",
            "it: 1020  | loss 6.06  | Δw: 1.646\n",
            "it: 1030  | loss 6.06  | Δw: 1.58\n",
            "it: 1040  | loss 6.12  | Δw: 1.496\n",
            "it: 1050  | loss 6.03  | Δw: 1.513\n",
            "it: 1060  | loss 6.2  | Δw: 1.732\n",
            "it: 1070  | loss 5.98  | Δw: 1.658\n",
            "it: 1080  | loss 6.03  | Δw: 1.654\n",
            "it: 1090  | loss 6.1  | Δw: 1.608\n",
            "it: 1100  | loss 6.1  | Δw: 1.673\n",
            "it: 1110  | loss 6.21  | Δw: 1.739\n",
            "it: 1120  | loss 6.14  | Δw: 1.778\n",
            "it: 1130  | loss 5.94  | Δw: 1.791\n",
            "it: 1140  | loss 6.08  | Δw: 1.805\n",
            "it: 1150  | loss 5.99  | Δw: 1.867\n",
            "it: 1160  | loss 6.1  | Δw: 1.841\n",
            "it: 1170  | loss 6.0  | Δw: 1.833\n",
            "it: 1180  | loss 5.93  | Δw: 1.894\n",
            "it: 1190  | loss 6.04  | Δw: 1.96\n",
            "it: 1200  | loss 6.0  | Δw: 2.073\n",
            "it: 1210  | loss 6.07  | Δw: 2.092\n",
            "it: 1220  | loss 5.99  | Δw: 2.035\n",
            "it: 1230  | loss 6.01  | Δw: 2.01\n",
            "it: 1240  | loss 5.92  | Δw: 2.181\n",
            "it: 1250  | loss 6.01  | Δw: 2.134\n",
            "it: 1260  | loss 6.05  | Δw: 2.145\n",
            "it: 1270  | loss 5.95  | Δw: 2.253\n",
            "it: 1280  | loss 6.01  | Δw: 2.165\n",
            "it: 1290  | loss 5.89  | Δw: 2.228\n",
            "it: 1300  | loss 5.91  | Δw: 2.299\n",
            "it: 1310  | loss 5.91  | Δw: 2.31\n",
            "it: 1320  | loss 6.03  | Δw: 2.265\n",
            "it: 1330  | loss 5.92  | Δw: 2.183\n",
            "it: 1340  | loss 5.89  | Δw: 2.191\n",
            "it: 1350  | loss 5.92  | Δw: 2.349\n",
            "it: 1360  | loss 5.99  | Δw: 2.384\n",
            "it: 1370  | loss 5.91  | Δw: 2.359\n",
            "it: 1380  | loss 5.95  | Δw: 2.365\n",
            "it: 1390  | loss 5.83  | Δw: 2.483\n",
            "it: 1400  | loss 5.91  | Δw: 2.778\n",
            "it: 1410  | loss 5.93  | Δw: 2.726\n",
            "it: 1420  | loss 5.92  | Δw: 2.632\n",
            "it: 1430  | loss 5.89  | Δw: 2.711\n",
            "it: 1440  | loss 5.81  | Δw: 2.607\n",
            "it: 1450  | loss 5.94  | Δw: 2.769\n",
            "it: 1460  | loss 5.92  | Δw: 2.797\n",
            "it: 1470  | loss 5.83  | Δw: 2.676\n",
            "it: 1480  | loss 5.87  | Δw: 2.694\n",
            "it: 1490  | loss 5.89  | Δw: 2.834\n",
            "it: 1500  | loss 5.74  | Δw: 3.17\n",
            "it: 1510  | loss 5.85  | Δw: 2.892\n",
            "it: 1520  | loss 5.81  | Δw: 2.927\n",
            "it: 1530  | loss 5.8  | Δw: 2.766\n",
            "it: 1540  | loss 5.8  | Δw: 2.872\n",
            "it: 1550  | loss 5.84  | Δw: 2.937\n",
            "it: 1560  | loss 5.8  | Δw: 3.095\n",
            "it: 1570  | loss 5.81  | Δw: 2.946\n",
            "it: 1580  | loss 5.82  | Δw: 3.001\n",
            "it: 1590  | loss 5.85  | Δw: 3.069\n",
            "it: 1600  | loss 5.75  | Δw: 2.936\n",
            "it: 1610  | loss 5.79  | Δw: 2.944\n",
            "it: 1620  | loss 5.82  | Δw: 3.158\n",
            "it: 1630  | loss 5.8  | Δw: 3.068\n",
            "it: 1640  | loss 5.82  | Δw: 3.088\n",
            "it: 1650  | loss 5.86  | Δw: 3.131\n",
            "it: 1660  | loss 5.84  | Δw: 3.303\n",
            "it: 1670  | loss 5.75  | Δw: 3.091\n",
            "it: 1680  | loss 5.78  | Δw: 3.35\n",
            "it: 1690  | loss 5.67  | Δw: 3.406\n",
            "it: 1700  | loss 5.74  | Δw: 3.282\n",
            "it: 1710  | loss 5.77  | Δw: 3.298\n",
            "it: 1720  | loss 5.77  | Δw: 3.403\n",
            "it: 1730  | loss 5.73  | Δw: 3.255\n",
            "it: 1740  | loss 5.71  | Δw: 3.458\n",
            "it: 1750  | loss 5.76  | Δw: 3.328\n",
            "it: 1760  | loss 5.6  | Δw: 3.299\n",
            "it: 1770  | loss 5.64  | Δw: 3.435\n",
            "it: 1780  | loss 5.69  | Δw: 3.378\n",
            "it: 1790  | loss 5.6  | Δw: 3.559\n",
            "it: 1800  | loss 5.61  | Δw: 3.635\n",
            "it: 1810  | loss 5.69  | Δw: 3.698\n",
            "it: 1820  | loss 5.74  | Δw: 3.719\n",
            "it: 1830  | loss 5.8  | Δw: 3.54\n",
            "it: 1840  | loss 5.54  | Δw: 3.659\n",
            "it: 1850  | loss 5.62  | Δw: 3.566\n",
            "it: 1860  | loss 5.64  | Δw: 3.643\n",
            "it: 1870  | loss 5.65  | Δw: 3.802\n",
            "it: 1880  | loss 5.62  | Δw: 3.675\n",
            "it: 1890  | loss 5.65  | Δw: 3.962\n",
            "it: 1900  | loss 5.64  | Δw: 3.777\n",
            "it: 1910  | loss 5.54  | Δw: 3.661\n",
            "it: 1920  | loss 5.63  | Δw: 3.786\n",
            "it: 1930  | loss 5.7  | Δw: 3.675\n",
            "it: 1940  | loss 5.57  | Δw: 3.93\n",
            "it: 1950  | loss 5.64  | Δw: 3.82\n",
            "it: 1960  | loss 5.61  | Δw: 3.83\n",
            "it: 1970  | loss 5.62  | Δw: 3.848\n",
            "it: 1980  | loss 5.65  | Δw: 4.008\n",
            "it: 1990  | loss 5.58  | Δw: 4.082\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# Results analysis\n",
        "# =============================================================================\n",
        "print('saving embeddings...')\n",
        "N = embed_save_limit\n",
        "np.savetxt('values.tsv', np.round(model.embeddings.weight.detach().cpu().numpy()[0:N], 2), delimiter='\\t', fmt='%1.2f')\n",
        "s = [dataset.rvocab[i] for i in range(N)]\n",
        "open('names.tsv', 'w+').write('\\n'.join(s) )\n",
        "print('end')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9ljEx8US8X-z",
        "outputId": "a63f8681-6df9-4345-ba5f-6513aa57d219"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "saving embeddings...\n",
            "end\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JaDsT6Jl9P3Z"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}